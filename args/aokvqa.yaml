name: run_1_aokvqa_baseline
model_id: gpt2
seed: 42
bf16: false  # Not using bf16; using fp16 (via AMP)
project: coconut_aokvqa_guna

# ‚úÖ Save checkpoints to Google Drive (persistent)
save_path: /content/drive/MyDrive/COCONUT/checkpoints/coconut_aokvqa_gunadeep
load_model_path: null
resume: 0
reset_optimizer: false
only_eval: false
save_only_improve: true

# ‚úÖ Dataset paths (adjust if needed)
train_path: /content/MultiModal-COCONUT-GPT2-CLIP-/data/Datasets/A-OKVQA/aokvqa_train.json
val_path: /content/MultiModal-COCONUT-GPT2-CLIP-/data/Datasets/A-OKVQA/aokvqa_validation.json

# ‚õìÔ∏è Reasoning setup
debug: false
no_thoughts: false
cot: false
no_cot: false
coconut: true

# üß† Curriculum configs
c_thought: 2
max_latent_stage: 8
epochs_per_stage: 2
pad_latent_to_max: true
uniform_prob: 0.1

# üß† Latent-space configs (REQUIRED for EM-style training)
latent_dim: 768           # GPT-2 base hidden size
n_latents: 4              # Number of latent tokens per sample
latent_lr: 1e-3           # Learning rate for latent vectors (E-step)
e_steps: 3                # Number of E-step optimization steps per batch

# üèãÔ∏è Training configs for Colab Free Tier
num_epochs: 25
batch_size_training: 32         # Increased for A100 GPU (was 8)
gradient_accumulation_steps: 2 # Simulates effective batch size of 2
lr: 5e-5
weight_decay: 0.0

max_length: 512  # Optimal for A-OKVQA, fits all data, saves memory
