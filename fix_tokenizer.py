#!/usr/bin/env python3
"""
Fix script for LLaVA tokenizer issues.
This script clears cached tokenizer files and tests the fix.
"""

import os
import shutil
from pathlib import Path

def clear_tokenizer_cache():
    """Clear cached tokenizer files that might be corrupted."""
    
    print("üßπ Clearing tokenizer cache...")
    
    # Common cache locations
    cache_paths = [
        os.path.expanduser("~/.cache/huggingface/hub"),
        "/tmp/huggingface_cache",
        "/content/.cache/huggingface/hub"
    ]
    
    cleared = False
    for cache_path in cache_paths:
        if os.path.exists(cache_path):
            print(f"üìÅ Found cache at: {cache_path}")
            try:
                # Look for LLaVA-related files
                for root, dirs, files in os.walk(cache_path):
                    for file in files:
                        if "llava" in file.lower() and file.endswith(".json"):
                            file_path = os.path.join(root, file)
                            print(f"üóëÔ∏è  Removing: {file_path}")
                            os.remove(file_path)
                            cleared = True
            except Exception as e:
                print(f"‚ö†Ô∏è  Could not clear cache at {cache_path}: {e}")
    
    if cleared:
        print("‚úÖ Tokenizer cache cleared")
    else:
        print("‚ÑπÔ∏è  No LLaVA cache files found to clear")
    
    return cleared

def test_fixed_loading():
    """Test the fixed tokenizer loading."""
    
    print("\nüß™ Testing fixed tokenizer loading...")
    
    try:
        from transformers import LlavaProcessor, LlamaTokenizer
        
        model_id = "llava-hf/llava-1.5-7b-hf"
        
        # Try the fixed loading method for transformers 4.37.2
        print("üìù Testing with minimal parameters...")
        processor = LlavaProcessor.from_pretrained(
            model_id, 
            trust_remote_code=True
        )
        tokenizer = processor.tokenizer
        print(f"‚úÖ SUCCESS! Tokenizer loaded with vocab size: {len(tokenizer)}")
        return True
        
    except Exception as e:
        print(f"‚ùå Still failing: {e}")
        return False

if __name__ == "__main__":
    print("üîß LLaVA Tokenizer Fix Script")
    print("=" * 40)
    
    # Clear cache
    clear_tokenizer_cache()
    
    # Test loading
    success = test_fixed_loading()
    
    print("\n" + "=" * 40)
    if success:
        print("üéâ Tokenizer fix successful!")
        print("‚úÖ You can now run: python test_model_loading.py")
    else:
        print("‚ùå Tokenizer fix failed")
        print("‚ö†Ô∏è  Try restarting the Colab runtime and running again") 